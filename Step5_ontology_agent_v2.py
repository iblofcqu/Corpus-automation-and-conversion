"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                   äº‹æ•…æŠ¥å‘Šæœ¬ä½“è®ºé©±åŠ¨çš„æ™ºèƒ½å¤„ç† Agent
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

æ¶æ„è¯´æ˜ï¼š
æœ¬ Agent ç³»ç»ŸåŸºäºæœ¬ä½“è®ºï¼ˆOntologyï¼‰è®¾è®¡ï¼Œå°†äº‹æ•…è°ƒæŸ¥æŠ¥å‘Šè½¬åŒ–ä¸ºå±‚çº§åŒ–çš„ç»“æ„ä¿¡æ¯ã€‚

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           AGENT æ¶æ„å…¨æ™¯å›¾                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   è¾“å…¥: Markdown æ–‡ä»¶                  â”‚
        â”‚            (å·²ç”± Step3_organize_by_headings_llm.py           â”‚
        â”‚                  é‡å†™æ ‡é¢˜å±‚çº§çš„æ–‡ä»¶)                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            1ï¸âƒ£  HeaderExtractor æ ‡é¢˜æå–å™¨               â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  åŠŸèƒ½: æå– MD æ–‡ä»¶çš„æ‰€æœ‰æ ‡é¢˜å±‚çº§                  â”‚  â”‚
        â”‚  â”‚  è¾“å‡º: [{level: 1, title: "...", content: "..."}] â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚  å­˜å…¥ Memory Pool
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               Memory Pool (è®°å¿†æ± )                       â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚ document_path:   æ–‡æ¡£è·¯å¾„                         â”‚  â”‚
        â”‚  â”‚ document_content: å®Œæ•´æ–‡æ¡£å†…å®¹                    â”‚  â”‚
        â”‚  â”‚ headers:         æå–çš„æ ‡é¢˜å±‚çº§åˆ—è¡¨                â”‚  â”‚
        â”‚  â”‚ split_plan:      LLMç”Ÿæˆçš„æ‹†åˆ†æ–¹æ¡ˆ                â”‚  â”‚
        â”‚  â”‚ chunks:          æ‹†åˆ†åçš„æ–‡æ¡£å—                    â”‚  â”‚
        â”‚  â”‚ extracted_data:  æå–çš„æœ¬ä½“è®ºæ•°æ®                 â”‚  â”‚
        â”‚  â”‚ ontology:        æœ¬ä½“è®ºç»“æ„                       â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚  è¯»å– ontology.json
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚            2ï¸âƒ£  SplitPlanner æ‹†åˆ†è§„åˆ’å™¨                  â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  åŠŸèƒ½: LLM æ ¹æ®æœ¬ä½“è®ºå’Œæ ‡é¢˜å±‚çº§ç”Ÿæˆæ‹†åˆ†æ–¹æ¡ˆ       â”‚  â”‚
        â”‚  â”‚  è¾“å…¥: headers + ontology                         â”‚  â”‚
        â”‚  â”‚  è¾“å‡º: æ‹†åˆ†æ–¹æ¡ˆ {                                 â”‚  â”‚
        â”‚  â”‚          "chunk_1": {                             â”‚  â”‚
        â”‚  â”‚             "ontology_category": "äº‹æ•…åŸºæœ¬æƒ…å†µ",  â”‚  â”‚
        â”‚  â”‚             "header_ranges": [[0, 5]],            â”‚  â”‚
        â”‚  â”‚             "reason": "..."                       â”‚  â”‚
        â”‚  â”‚          }                                        â”‚  â”‚
        â”‚  â”‚        }                                          â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚  å­˜å…¥ Memory Pool
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           3ï¸âƒ£  DocumentSplitter æ–‡æ¡£æ‹†åˆ†å™¨               â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  åŠŸèƒ½: æ ¹æ®æ‹†åˆ†æ–¹æ¡ˆæ‰§è¡Œæ–‡æ¡£åˆ‡åˆ†                   â”‚  â”‚
        â”‚  â”‚  è¾“å…¥: split_plan + document_content              â”‚  â”‚
        â”‚  â”‚  è¾“å‡º: chunks = [{                                â”‚  â”‚
        â”‚  â”‚           "chunk_id": "chunk_1",                  â”‚  â”‚
        â”‚  â”‚           "ontology_category": "äº‹æ•…åŸºæœ¬æƒ…å†µ",    â”‚  â”‚
        â”‚  â”‚           "content": "å®Œæ•´çš„åŸæ–‡å†…å®¹...",         â”‚  â”‚
        â”‚  â”‚           "headers_included": [...]               â”‚  â”‚
        â”‚  â”‚         }]                                        â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚  å­˜å…¥ Memory Pool
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        4ï¸âƒ£  InformationExtractor ä¿¡æ¯æå–å™¨              â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  åŠŸèƒ½: ä¸¥æ ¼æŒ‰ç…§æœ¬ä½“è®ºæå–ä¿¡æ¯(å¤åˆ¶åŸæ–‡,ä¸æ”¹å†™)   â”‚  â”‚
        â”‚  â”‚  ç­–ç•¥:                                            â”‚  â”‚
        â”‚  â”‚    - copy_exact: ç²¾ç¡®å¤åˆ¶å­—æ®µå€¼                   â”‚  â”‚
        â”‚  â”‚    - copy_section: å¤åˆ¶æ•´æ®µå†…å®¹                   â”‚  â”‚
        â”‚  â”‚    - list_extract: é€æ¡å¤åˆ¶åˆ—è¡¨                   â”‚  â”‚
        â”‚  â”‚    - structured_extract: æŒ‰schemaå¤åˆ¶å¯¹è±¡         â”‚  â”‚
        â”‚  â”‚    - structured_list_extract: æŒ‰schemaå¤åˆ¶åˆ—è¡¨    â”‚  â”‚
        â”‚  â”‚                                                   â”‚  â”‚
        â”‚  â”‚  è¾“å…¥: chunk + ontology_category                  â”‚  â”‚
        â”‚  â”‚  è¾“å‡º: æå–çš„ç»“æ„åŒ–æ•°æ®                           â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚  å­˜å…¥ Memory Pool
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         5ï¸âƒ£  OntologySerializer åºåˆ—åŒ–å™¨                 â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚  åŠŸèƒ½: æŒ‰æœ¬ä½“è®ºç»“æ„ç»„ç»‡æ•°æ®å¹¶åºåˆ—åŒ–ä¸ºJSON         â”‚  â”‚
        â”‚  â”‚  è¾“å…¥: extracted_data (å„chunkæå–çš„æ•°æ®)         â”‚  â”‚
        â”‚  â”‚  è¾“å‡º: ç¬¦åˆæœ¬ä½“è®ºç»“æ„çš„å®Œæ•´JSONæ–‡ä»¶               â”‚  â”‚
        â”‚  â”‚  {                                                â”‚  â”‚
        â”‚  â”‚    "æŠ¥å‘Šå…ƒä¿¡æ¯": {...},                           â”‚  â”‚
        â”‚  â”‚    "äº‹æ•…åŸºæœ¬æƒ…å†µ": {...},                         â”‚  â”‚
        â”‚  â”‚    "äº‹æ•…ç»è¿‡ä¸æ€§è´¨": {...},                       â”‚  â”‚
        â”‚  â”‚    "äººå‘˜ä¼¤äº¡æƒ…å†µ": {...},                         â”‚  â”‚
        â”‚  â”‚    "äº‹æ•…åŸå› åˆ†æ": {...},                         â”‚  â”‚
        â”‚  â”‚    "è´£ä»»è®¤å®š": {...}                              â”‚  â”‚
        â”‚  â”‚  }                                                â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                è¾“å‡º: JSON æ–‡ä»¶                           â”‚
        â”‚           (ç¬¦åˆæœ¬ä½“è®ºç»“æ„çš„äº‹æ•…æŠ¥å‘Šæ•°æ®)                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         æ ¸å¿ƒè®¾è®¡åŸåˆ™                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. æœ¬ä½“è®ºé©±åŠ¨ (Ontology-Driven)
   - æœ¬ä½“è®ºå®šä¹‰äº†äº‹æ•…æŠ¥å‘Šçš„æ ‡å‡†åˆ†ææ¡†æ¶
   - æ‰€æœ‰æå–æ“ä½œä¸¥æ ¼éµå¾ªæœ¬ä½“è®ºå®šä¹‰çš„å­—æ®µå’Œç­–ç•¥

2. ä¸¥æ ¼åŸæ–‡å¤åˆ¶ (Exact Copy)
   - æå–ç­–ç•¥å¼ºè°ƒ "copy_exact" è€Œé "generate"
   - LLM çš„è§’è‰²æ˜¯ "å®šä½å’Œå¤åˆ¶"ï¼Œè€Œé "ç†è§£å’Œæ”¹å†™"
   - ä¿è¯æ•°æ®çš„åŸå§‹æ€§å’Œå‡†ç¡®æ€§

3. è®°å¿†æ± è®¾è®¡ (Memory Pool)
   - å„æ¨¡å—é—´é€šè¿‡ç»Ÿä¸€çš„è®°å¿†æ± ä¼ é€’æ•°æ®
   - æ”¯æŒæ•°æ®è¿½æº¯å’Œè°ƒè¯•
   - ä¾¿äºæ‰©å±•æ–°çš„å¤„ç†æ¨¡å—

4. ç­–ç•¥å¯é…ç½® (Strategy Configurable)
   - æå–ç­–ç•¥åœ¨ ontology.json ä¸­é…ç½®
   - æ”¯æŒè‡ªå®šä¹‰ prompt_template
   - çµæ´»é€‚åº”ä¸åŒç±»å‹çš„å­—æ®µæå–éœ€æ±‚

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ä½œè€…: Zijin Qiu
ç‰ˆæœ¬: v1.0
æ—¥æœŸ: 2025-10-26
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
from openai import OpenAI
import copy


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                           LLM é…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODEL = "deepseek-chat"
API_KEY = 'sk-4f3ca5dd06a447aeb81989119aa197c6'
BASE_URL = "https://api.deepseek.com"

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                       Memory Pool (è®°å¿†æ± )
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MemoryPool:
    """
    è®°å¿†æ± ï¼šç”¨äºåœ¨ Agent å„æ¨¡å—é—´ä¼ é€’å’Œå­˜å‚¨æ•°æ®

    è®¾è®¡ç›®çš„ï¼š
    1. ç»Ÿä¸€çš„æ•°æ®å­˜å‚¨æ¥å£
    2. æ”¯æŒæ•°æ®ç‰ˆæœ¬è¿½æº¯
    3. ä¾¿äºè°ƒè¯•å’Œæ—¥å¿—è®°å½•
    """

    def __init__(self):
        self.memory = {
            "document_path": None,           # æ–‡æ¡£è·¯å¾„
            "document_content": None,        # å®Œæ•´æ–‡æ¡£å†…å®¹
            "headers": [],                   # æå–çš„æ ‡é¢˜å±‚çº§
            "ontology": None,                # æœ¬ä½“è®ºç»“æ„
            "split_plan": None,              # æ‹†åˆ†æ–¹æ¡ˆ
            "chunks": [],                    # æ‹†åˆ†åçš„æ–‡æ¡£å—
            "extracted_data": {},            # æå–çš„æ•°æ®
            "processing_log": [],            # å¤„ç†æ—¥å¿—
        }

    def set(self, key: str, value: Any):
        """å­˜å‚¨æ•°æ®åˆ°è®°å¿†æ± """
        self.memory[key] = value
        self.log(f"Memory updated: {key}")

    def get(self, key: str) -> Any:
        """ä»è®°å¿†æ± è·å–æ•°æ®"""
        return self.memory.get(key)

    def log(self, message: str):
        """è®°å½•å¤„ç†æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.memory["processing_log"].append(log_entry)
        print(f"  ğŸ“ {message}")

    def _convert_to_serializable(self, obj):
        """
        é€’å½’è½¬æ¢å¯¹è±¡ä¸ºå¯åºåˆ—åŒ–æ ¼å¼

        å¤„ç†:
        - Path å¯¹è±¡ â†’ å­—ç¬¦ä¸²
        - å…¶ä»–ä¸å¯åºåˆ—åŒ–å¯¹è±¡ â†’ å­—ç¬¦ä¸²è¡¨ç¤º
        """
        if isinstance(obj, Path):
            return str(obj)
        elif isinstance(obj, dict):
            return {key: self._convert_to_serializable(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_to_serializable(item) for item in obj]
        elif isinstance(obj, (str, int, float, bool, type(None))):
            return obj
        else:
            # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            return str(obj)

    def save_memory(self, output_path: str):
        """ä¿å­˜è®°å¿†æ± åˆ°æ–‡ä»¶ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
        # åˆ›å»ºå¯åºåˆ—åŒ–çš„å‰¯æœ¬
        serializable_memory = copy.deepcopy(self.memory)

        # è½¬æ¢æ‰€æœ‰ä¸å¯åºåˆ—åŒ–å¯¹è±¡
        serializable_memory = self._convert_to_serializable(serializable_memory)

        # æˆªæ–­è¿‡é•¿çš„å†…å®¹
        if serializable_memory.get("document_content"):
            content = serializable_memory["document_content"]
            if isinstance(content, str) and len(content) > 1000:
                serializable_memory["document_content"] = content[:1000] + "...(truncated)"

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_memory, f, ensure_ascii=False, indent=2)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                   1ï¸âƒ£ HeaderExtractor (æ ‡é¢˜æå–å™¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HeaderExtractor:
    """
    æ ‡é¢˜æå–å™¨ï¼šä» Markdown æ–‡ä»¶ä¸­æå–æ ‡é¢˜å±‚çº§ç»“æ„

    è¾“å…¥: Markdown æ–‡ä»¶å†…å®¹
    è¾“å‡º: [{level: 1, title: "...", content: "...", start_line: 0, end_line: 10}, ...]
    """

    @staticmethod
    def extract(md_content: str, memory_pool: MemoryPool) -> List[Dict]:
        """
        æå–æ ‡é¢˜å±‚çº§

        Args:
            md_content: Markdown æ–‡ä»¶å†…å®¹
            memory_pool: è®°å¿†æ± 

        Returns:
            æ ‡é¢˜åˆ—è¡¨
        """
        memory_pool.log("HeaderExtractor: å¼€å§‹æå–æ ‡é¢˜å±‚çº§")

        lines = md_content.split('\n')
        headers = []
        current_header = None
        first_header_line = None  # è®°å½•ç¬¬ä¸€ä¸ªæ ‡é¢˜çš„è¡Œå·

        # å…ˆæ‰¾åˆ°ç¬¬ä¸€ä¸ªæ ‡é¢˜çš„ä½ç½®
        for i, line in enumerate(lines):
            if line.strip().startswith('#'):
                first_header_line = i
                break

        # å¦‚æœç¬¬ä¸€ä¸ªæ ‡é¢˜ä¹‹å‰æœ‰å†…å®¹ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„"æ–‡æ¡£å¼€å¤´"æ ‡é¢˜
        if first_header_line is not None and first_header_line > 0:
            preamble_content = '\n'.join(lines[0:first_header_line]).strip()
            if preamble_content:  # åªæœ‰éç©ºå†…å®¹æ‰æ·»åŠ 
                headers.append({
                    'index': 0,
                    'level': 0,
                    'title': 'æ–‡æ¡£å¼€å¤´ï¼ˆåŸºæœ¬ä¿¡æ¯ï¼‰',
                    'start_line': 0,
                    'end_line': first_header_line - 1,
                    'content': preamble_content
                })
                memory_pool.log("HeaderExtractor: æ£€æµ‹åˆ°æ–‡æ¡£å¼€å¤´æœ‰åŸºæœ¬ä¿¡æ¯ï¼ˆä¸åœ¨æ ‡é¢˜å±‚çº§ä¸‹ï¼‰")

        for i, line in enumerate(lines):
            line_stripped = line.strip()

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡é¢˜è¡Œ
            if line_stripped.startswith('#'):
                # ä¿å­˜ä¸Šä¸€ä¸ªæ ‡é¢˜
                if current_header:
                    current_header['end_line'] = i - 1
                    current_header['content'] = '\n'.join(lines[current_header['start_line']:i]).strip()
                    headers.append(current_header)

                # åˆ›å»ºæ–°æ ‡é¢˜
                level = len(line_stripped) - len(line_stripped.lstrip('#'))
                title = line_stripped.lstrip('#').strip()

                current_header = {
                    'index': len(headers),
                    'level': level,
                    'title': title,
                    'start_line': i,
                    'end_line': None,
                    'content': ''
                }

        # ä¿å­˜æœ€åä¸€ä¸ªæ ‡é¢˜
        if current_header:
            current_header['end_line'] = len(lines) - 1
            current_header['content'] = '\n'.join(lines[current_header['start_line']:]).strip()
            headers.append(current_header)

        memory_pool.log(f"HeaderExtractor: æå–åˆ° {len(headers)} ä¸ªæ ‡é¢˜")
        memory_pool.set("headers", headers)

        return headers


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                   2ï¸âƒ£ SplitPlanner (æ‹†åˆ†è§„åˆ’å™¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SplitPlanner:
    """
    æ‹†åˆ†è§„åˆ’å™¨ï¼šæ ¹æ®æœ¬ä½“è®ºå’Œæ ‡é¢˜å±‚çº§ï¼Œä½¿ç”¨ LLM ç”Ÿæˆæ–‡æ¡£æ‹†åˆ†æ–¹æ¡ˆ

    è¾“å…¥: æ ‡é¢˜åˆ—è¡¨ + æœ¬ä½“è®ºç»“æ„
    è¾“å‡º: æ‹†åˆ†æ–¹æ¡ˆ
    """

    @staticmethod
    def plan(memory_pool: MemoryPool) -> Dict:
        """
        ç”Ÿæˆæ‹†åˆ†æ–¹æ¡ˆ

        Args:
            memory_pool: è®°å¿†æ± 

        Returns:
            æ‹†åˆ†æ–¹æ¡ˆ
        """
        memory_pool.log("SplitPlanner: å¼€å§‹è§„åˆ’æ–‡æ¡£æ‹†åˆ†æ–¹æ¡ˆ")

        headers = memory_pool.get("headers")
        ontology = memory_pool.get("ontology")

        # æ„å»ºæœ¬ä½“è®ºç±»åˆ«æ‘˜è¦
        ontology_summary = []
        for category_name, category_info in ontology["ontology_structure"].items():
            ontology_summary.append({
                "ç±»åˆ«åç§°": category_name,
                "ä¼˜å…ˆçº§": category_info["priority"],
                "æè¿°": category_info["description"],
                "å…³é”®è¯": category_info["keywords"],
                "æœ€å¤§tokens": category_info.get("max_tokens", 5000)
            })

        # æ„å»ºæ ‡é¢˜æ‘˜è¦
        headers_summary = []
        for h in headers:
            headers_summary.append({
                "ç´¢å¼•": h["index"],
                "å±‚çº§": h["level"],
                "æ ‡é¢˜": h["title"],
                "å†…å®¹é•¿åº¦": len(h["content"])
            })

        # æ„å»º LLM prompt
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äº‹æ•…æŠ¥å‘Šåˆ†æä¸“å®¶ã€‚ç°åœ¨éœ€è¦ä½ æ ¹æ®æœ¬ä½“è®ºç»“æ„å’Œæ–‡æ¡£çš„æ ‡é¢˜å±‚çº§ï¼Œåˆ¶å®šä¸€ä¸ªæ–‡æ¡£æ‹†åˆ†æ–¹æ¡ˆã€‚

ã€æœ¬ä½“è®ºç»“æ„ã€‘ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
{json.dumps(ontology_summary, ensure_ascii=False, indent=2)}

ã€æ–‡æ¡£æ ‡é¢˜å±‚çº§ã€‘
{json.dumps(headers_summary, ensure_ascii=False, indent=2)}

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. å°†æ–‡æ¡£æ ‡é¢˜åˆ†é…åˆ°æœ¬ä½“è®ºçš„å„ä¸ªç±»åˆ«ä¸­
2. æ¯ä¸ªç±»åˆ«å¯ä»¥åŒ…å«å¤šä¸ªæ ‡é¢˜ï¼ˆé€šè¿‡æ ‡é¢˜ç´¢å¼•æŒ‡å®šèŒƒå›´ï¼‰
3. è€ƒè™‘æ ‡é¢˜çš„è¯­ä¹‰å’Œæœ¬ä½“è®ºç±»åˆ«çš„å…³é”®è¯åŒ¹é…
4. æ§åˆ¶æ¯ä¸ªchunkçš„å¤§å°ä¸è¶…è¿‡ç±»åˆ«çš„max_tokens
5. ä¼˜å…ˆçº§é«˜çš„ç±»åˆ«ä¼˜å…ˆåˆ†é…

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "chunk_åŸºæœ¬æƒ…å†µ": {{
    "ontology_category": "äº‹æ•…åŸºæœ¬æƒ…å†µ",
    "header_indices": [0, 1, 2],
    "reason": "è¿™äº›æ ‡é¢˜åŒ…å«äº†å·¥ç¨‹æ¦‚å†µã€é¡¹ç›®ä¿¡æ¯ç­‰åŸºæœ¬æƒ…å†µ"
  }},
  "chunk_äº‹æ•…ç»è¿‡": {{
    "ontology_category": "äº‹æ•…ç»è¿‡ä¸æ€§è´¨",
    "header_indices": [3, 4],
    "reason": "è¿™äº›æ ‡é¢˜æè¿°äº†äº‹æ•…çš„å‘ç”Ÿè¿‡ç¨‹"
  }},
  ...
}}

**é‡è¦ï¼šåªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šæ–‡å­—ã€‚**
"""

        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„äº‹æ•…æŠ¥å‘Šåˆ†æä¸“å®¶ã€‚åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡æœ¬ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )

            content_str = response.choices[0].message.content

            # è§£æ JSON
            try:
                split_plan = json.loads(content_str)
            except json.JSONDecodeError:
                # å°è¯•æå– JSON éƒ¨åˆ†
                code_block_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', content_str, re.DOTALL)
                if code_block_match:
                    split_plan = json.loads(code_block_match.group(1))
                else:
                    json_match = re.search(r'\{.*\}', content_str, re.DOTALL)
                    if json_match:
                        split_plan = json.loads(json_match.group())
                    else:
                        raise ValueError("æ— æ³•è§£æ LLM è¿”å›çš„æ‹†åˆ†æ–¹æ¡ˆ")

            memory_pool.log(f"SplitPlanner: ç”Ÿæˆäº† {len(split_plan)} ä¸ªæ‹†åˆ†chunk")
            memory_pool.set("split_plan", split_plan)

            return split_plan

        except Exception as e:
            memory_pool.log(f"SplitPlanner: é”™è¯¯ - {e}")
            raise


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                   3ï¸âƒ£ DocumentSplitter (æ–‡æ¡£æ‹†åˆ†å™¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DocumentSplitter:
    """
    æ–‡æ¡£æ‹†åˆ†å™¨ï¼šæ ¹æ®æ‹†åˆ†æ–¹æ¡ˆæ‰§è¡Œæ–‡æ¡£åˆ‡åˆ†

    è¾“å…¥: æ‹†åˆ†æ–¹æ¡ˆ + æ ‡é¢˜åˆ—è¡¨
    è¾“å‡º: æ‹†åˆ†åçš„æ–‡æ¡£å—
    """

    @staticmethod
    def split(memory_pool: MemoryPool) -> List[Dict]:
        """
        æ‰§è¡Œæ–‡æ¡£æ‹†åˆ†

        Args:
            memory_pool: è®°å¿†æ± 

        Returns:
            æ–‡æ¡£å—åˆ—è¡¨
        """
        memory_pool.log("DocumentSplitter: å¼€å§‹æ‹†åˆ†æ–‡æ¡£")

        split_plan = memory_pool.get("split_plan")
        headers = memory_pool.get("headers")

        chunks = []

        for chunk_id, chunk_info in split_plan.items():
            ontology_category = chunk_info["ontology_category"]
            header_indices = chunk_info["header_indices"]

            # åˆå¹¶æŒ‡å®šç´¢å¼•çš„æ ‡é¢˜å†…å®¹
            content_parts = []
            headers_included = []

            for idx in header_indices:
                if 0 <= idx < len(headers):
                    header = headers[idx]
                    content_parts.append(header["content"])
                    headers_included.append({
                        "index": header["index"],
                        "level": header["level"],
                        "title": header["title"]
                    })

            chunk = {
                "chunk_id": chunk_id,
                "ontology_category": ontology_category,
                "content": "\n\n".join(content_parts),
                "headers_included": headers_included,
                "char_count": sum(len(part) for part in content_parts)
            }

            chunks.append(chunk)
            memory_pool.log(f"DocumentSplitter: åˆ›å»ºchunk '{chunk_id}' (ç±»åˆ«: {ontology_category}, {chunk['char_count']} å­—ç¬¦)")

        memory_pool.set("chunks", chunks)
        return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                   4ï¸âƒ£ InformationExtractor (ä¿¡æ¯æå–å™¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class InformationExtractor:
    """
    ä¿¡æ¯æå–å™¨ï¼šä¸¥æ ¼æŒ‰ç…§æœ¬ä½“è®ºæå–ä¿¡æ¯ï¼Œå¼ºè°ƒåŸæ–‡å¤åˆ¶è€Œéæ”¹å†™

    æ ¸å¿ƒç­–ç•¥ï¼š
    - copy_exact: ç²¾ç¡®å¤åˆ¶å­—æ®µå€¼
    - copy_section: å¤åˆ¶æ•´æ®µå†…å®¹
    - list_extract: é€æ¡å¤åˆ¶åˆ—è¡¨
    - structured_extract: æŒ‰schemaå¤åˆ¶å¯¹è±¡
    - structured_list_extract: æŒ‰schemaå¤åˆ¶åˆ—è¡¨
    - cross_chunk_summarize: è·¨chunkç»¼åˆæ€»ç»“
    - cross_chunk_list_extract: è·¨chunkåˆ—è¡¨æå–
    - cross_chunk_structured_list_extract: è·¨chunkç»“æ„åŒ–åˆ—è¡¨æå–
    - classify_with_options: ä»é¢„å®šä¹‰é€‰é¡¹ä¸­åˆ†ç±»é€‰æ‹©
    """

    # é¢„å®šä¹‰çš„åˆ†ç±»é€‰é¡¹
    ACCIDENT_LEVEL_OPTIONS = ["ä¸€èˆ¬", "è¾ƒå¤§", "é‡å¤§", "ç‰¹åˆ«é‡å¤§"]
    ACCIDENT_NATURE_OPTIONS = ["è´£ä»»äº‹æ•…", "æ„å¤–(éè´£ä»»)äº‹æ•…"]

    @staticmethod
    def _extract_responsible_persons(memory_pool: MemoryPool, ontology: Dict) -> List[Dict]:
        """
        æå–è´£ä»»äººå‘˜ä¿¡æ¯ï¼Œç»¼åˆäººå‘˜ä¼¤äº¡æƒ…å†µå’Œè´£ä»»è®¤å®šä¸¤éƒ¨åˆ†

        Args:
            memory_pool: è®°å¿†æ± 
            ontology: æœ¬ä½“è®º

        Returns:
            è´£ä»»äººå‘˜åˆ—è¡¨
        """
        memory_pool.log("    è·¨chunkæå–è´£ä»»äººå‘˜ï¼Œç»¼åˆäººå‘˜ä¼¤äº¡æƒ…å†µå’Œè´£ä»»è®¤å®š")

        # æ”¶é›†äººå‘˜ä¼¤äº¡æƒ…å†µå†…å®¹
        casualties_content = InformationExtractor._collect_cross_chunk_content(
            ["äººå‘˜ä¼¤äº¡æƒ…å†µ"], memory_pool
        )

        # æ”¶é›†è´£ä»»è®¤å®šå†…å®¹
        responsibility_content = InformationExtractor._collect_cross_chunk_content(
            ["è´£ä»»è®¤å®š"], memory_pool
        )

        # åˆå¹¶ä¸¤éƒ¨åˆ†å†…å®¹
        combined_content = f"""ã€äººå‘˜åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†ã€‘
{casualties_content}

ã€è´£ä»»è®¤å®šéƒ¨åˆ†ã€‘
{responsibility_content}"""

        # è·å–è´£ä»»äººå‘˜çš„schema
        schema = {
            "å§“å": {"type": "string", "extraction_strategy": "copy_exact"},
            "æ€§åˆ«": {"type": "string", "extraction_strategy": "copy_exact"},
            "å¹´é¾„": {"type": "string", "extraction_strategy": "copy_exact"},
            "èŒä½æˆ–å·¥ç§": {"type": "string", "extraction_strategy": "copy_exact"},
            "æŒè¯ä¸Šå²—æƒ…å†µ": {"type": "string", "extraction_strategy": "copy_exact"},
            "æ‰€å±å•ä½": {"type": "string", "extraction_strategy": "copy_exact"},
            "è´£ä»»è®¤å®š": {"type": "text", "extraction_strategy": "copy_section"},
            "å¤„ç½šæ„è§": {"type": "text", "extraction_strategy": "copy_section"}
        }

        schema_str = json.dumps(schema, ensure_ascii=False, indent=2)

        prompt = f"""æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œæå–æ‰€æœ‰äº‹æ•…è´£ä»»äººå‘˜çš„ä¿¡æ¯ã€‚è¦æ±‚ï¼š

1. äººå‘˜åŸºæœ¬ä¿¡æ¯(å§“åã€æ€§åˆ«ã€å¹´é¾„ã€èŒä½æˆ–å·¥ç§ã€æ‰€å±å•ä½)ä¸»è¦ä»ã€äººå‘˜åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†ã€‘ä¸­è·å–
2. è´£ä»»è®¤å®šå’Œå¤„ç½šæ„è§ä»ã€è´£ä»»è®¤å®šéƒ¨åˆ†ã€‘ä¸­è·å–
3. éœ€è¦å°†ä¸¤éƒ¨åˆ†ä¿¡æ¯å…³è”èµ·æ¥ï¼ˆé€šè¿‡å§“ååŒ¹é…ï¼‰
4. åªæå–è¢«è®¤å®šæœ‰è´£ä»»çš„äººå‘˜ï¼ˆåœ¨è´£ä»»è®¤å®šéƒ¨åˆ†æœ‰æ˜ç¡®è¯´æ˜çš„äººå‘˜ï¼‰
5. æ¯ä¸ªäººå‘˜ä¿¡æ¯æŒ‰ç…§ä»¥ä¸‹schemaæå–ï¼š

{schema_str}

6. ç›´æ¥å¤åˆ¶åŸæ–‡ä¸­çš„å­—æ®µå€¼ï¼Œä¸è¦æ”¹å†™æˆ–æ€»ç»“
7. ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›

æ–‡æ¡£å†…å®¹ï¼š
{combined_content[:15000]}

è¿”å›æ ¼å¼ï¼š[{{"å§“å": "åŸæ–‡å€¼1", "æ€§åˆ«": "åŸæ–‡å€¼", ...}}, {{"å§“å": "åŸæ–‡å€¼2", ...}}, ...]"""

        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚ç»¼åˆåˆ†æäººå‘˜åŸºæœ¬ä¿¡æ¯å’Œè´£ä»»è®¤å®šä¸¤éƒ¨åˆ†å†…å®¹ï¼Œæå–è´£ä»»äººå‘˜çš„å®Œæ•´ä¿¡æ¯ã€‚ä¸¥æ ¼å¤åˆ¶åŸæ–‡ï¼Œä¸è¦æ”¹å†™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=3000
            )

            result_str = response.choices[0].message.content.strip()

            # è§£æ JSON
            try:
                result = json.loads(result_str)
            except json.JSONDecodeError:
                # å°è¯•æå– JSON éƒ¨åˆ†
                code_block_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', result_str, re.DOTALL)
                if code_block_match:
                    result = json.loads(code_block_match.group(1))
                else:
                    json_match = re.search(r'\[.*\]', result_str, re.DOTALL)
                    if json_match:
                        result = json.loads(json_match.group())
                    else:
                        memory_pool.log(f"    è­¦å‘Š: æ— æ³•è§£æJSONç»“æœ")
                        result = []

            memory_pool.log(f"    æå–åˆ° {len(result)} ä¸ªè´£ä»»äººå‘˜")
            return result

        except Exception as e:
            memory_pool.log(f"    é”™è¯¯: è´£ä»»äººå‘˜æå–å¤±è´¥ - {e}")
            return []

    @staticmethod
    def _classify_with_options(content: str, field_name: str, options: List[str], memory_pool: MemoryPool) -> str:
        """
        ä»é¢„å®šä¹‰é€‰é¡¹ä¸­åˆ†ç±»é€‰æ‹©

        Args:
            content: æ–‡æ¡£å†…å®¹
            field_name: å­—æ®µåç§°
            options: é¢„å®šä¹‰é€‰é¡¹åˆ—è¡¨
            memory_pool: è®°å¿†æ± 

        Returns:
            é€‰ä¸­çš„é€‰é¡¹
        """
        prompt = f"""ä»ä»¥ä¸‹æ–‡æœ¬ä¸­è¯†åˆ«ã€Œ{field_name}ã€ï¼Œå¹¶ä»é¢„å®šä¹‰é€‰é¡¹ä¸­é€‰æ‹©æœ€åŒ¹é…çš„ä¸€é¡¹ã€‚

é¢„å®šä¹‰é€‰é¡¹ï¼š{', '.join(options)}

æ–‡æœ¬å†…å®¹ï¼š
{content[:8000]}

è¦æ±‚ï¼š
1. ä»”ç»†é˜…è¯»æ–‡æœ¬ï¼Œæ‰¾åˆ°ä¸ã€Œ{field_name}ã€ç›¸å…³çš„æè¿°
2. ä»é¢„å®šä¹‰é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªæœ€åŒ¹é…çš„é€‰é¡¹
3. åªè¿”å›é€‰é¡¹æœ¬èº«ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–å…¶ä»–æ–‡å­—
4. å¦‚æœæ–‡æœ¬ä¸­æ²¡æœ‰æ˜ç¡®è¯´æ˜ï¼Œè¯·æ ¹æ®æè¿°æ¨æ–­æœ€åˆç†çš„é€‰é¡¹

åªè¿”å›é€‰ä¸­çš„é€‰é¡¹ï¼š"""

        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": f"ä½ æ˜¯ä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚ä»ç»™å®šçš„é¢„å®šä¹‰é€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š{', '.join(options)}ã€‚åªè¿”å›é€‰é¡¹æœ¬èº«ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,
                max_tokens=50
            )

            result = response.choices[0].message.content.strip()

            # éªŒè¯ç»“æœæ˜¯å¦åœ¨é€‰é¡¹ä¸­
            for option in options:
                if option in result:
                    memory_pool.log(f"    åˆ†ç±»ç»“æœ: {option}")
                    return option

            # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€ä¸ªé€‰é¡¹ä½œä¸ºé»˜è®¤å€¼
            memory_pool.log(f"    è­¦å‘Š: åˆ†ç±»ç»“æœ '{result}' ä¸åœ¨é¢„å®šä¹‰é€‰é¡¹ä¸­ï¼Œä½¿ç”¨é»˜è®¤å€¼: {options[0]}")
            return options[0]

        except Exception as e:
            memory_pool.log(f"    é”™è¯¯: åˆ†ç±»å¤±è´¥ - {e}")
            return options[0]  # è¿”å›é»˜è®¤å€¼

    @staticmethod
    def _collect_cross_chunk_content(source_categories: List[str], memory_pool: MemoryPool) -> str:
        """
        æ”¶é›†å¤šä¸ªç±»åˆ«çš„chunkå†…å®¹

        Args:
            source_categories: éœ€è¦æ”¶é›†çš„æœ¬ä½“è®ºç±»åˆ«åˆ—è¡¨
            memory_pool: è®°å¿†æ± 

        Returns:
            åˆå¹¶åçš„å†…å®¹
        """
        chunks = memory_pool.get("chunks")
        collected_content = []

        for category in source_categories:
            # æŸ¥æ‰¾å±äºè¯¥ç±»åˆ«çš„æ‰€æœ‰chunk
            for chunk in chunks:
                if chunk["ontology_category"] == category:
                    collected_content.append(f"ã€{category}ã€‘\n{chunk['content']}")

        if not collected_content:
            memory_pool.log(f"    è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•æºç±»åˆ«çš„å†…å®¹")
            return ""

        # åˆå¹¶å†…å®¹ï¼Œé™åˆ¶æ€»é•¿åº¦
        merged_content = "\n\n".join(collected_content)
        max_length = 12000  # å¢åŠ é•¿åº¦ä»¥å®¹çº³å¤šä¸ªchunk

        if len(merged_content) > max_length:
            memory_pool.log(f"    æç¤º: å†…å®¹è¿‡é•¿({len(merged_content)}å­—ç¬¦)ï¼Œæˆªå–å‰{max_length}å­—ç¬¦")
            merged_content = merged_content[:max_length]

        return merged_content

    @staticmethod
    def extract(memory_pool: MemoryPool) -> Dict:
        """
        æå–ä¿¡æ¯

        Args:
            memory_pool: è®°å¿†æ± 

        Returns:
            æå–çš„æ•°æ®
        """
        memory_pool.log("InformationExtractor: å¼€å§‹æå–ä¿¡æ¯")

        chunks = memory_pool.get("chunks")
        ontology = memory_pool.get("ontology")

        extracted_data = {}
        processed_categories = set()  # è®°å½•å·²å¤„ç†çš„ç±»åˆ«

        for chunk in chunks:
            chunk_id = chunk["chunk_id"]
            ontology_category = chunk["ontology_category"]
            content = chunk["content"]

            # å¦‚æœè¯¥ç±»åˆ«å·²ç»å¤„ç†è¿‡ï¼Œè·³è¿‡ï¼ˆé¿å…é‡å¤æå–ï¼‰
            if ontology_category in processed_categories:
                memory_pool.log(f"InformationExtractor: è·³è¿‡å·²å¤„ç†çš„ç±»åˆ« '{ontology_category}'")
                continue

            memory_pool.log(f"InformationExtractor: å¤„ç†chunk '{chunk_id}' (ç±»åˆ«: {ontology_category})")

            # è·å–æœ¬ä½“è®ºç±»åˆ«å®šä¹‰
            category_def = ontology["ontology_structure"].get(ontology_category)
            if not category_def:
                memory_pool.log(f"InformationExtractor: è­¦å‘Š - æœªæ‰¾åˆ°æœ¬ä½“è®ºç±»åˆ« '{ontology_category}'")
                continue

            # æå–è¯¥ç±»åˆ«çš„æ‰€æœ‰å­—æ®µ
            category_data = {}

            for field_name, field_def in category_def["fields"].items():
                field_type = field_def["type"]
                extraction_strategy = field_def["extraction_strategy"]

                memory_pool.log(f"  æå–å­—æ®µ: {field_name} (ç­–ç•¥: {extraction_strategy})")

                # æ ¹æ®ç­–ç•¥æå–
                extracted_value = InformationExtractor._extract_field(
                    field_name=field_name,
                    field_def=field_def,
                    content=content,
                    ontology=ontology,
                    memory_pool=memory_pool
                )

                category_data[field_name] = extracted_value

            extracted_data[ontology_category] = category_data
            processed_categories.add(ontology_category)  # æ ‡è®°ä¸ºå·²å¤„ç†

        memory_pool.set("extracted_data", extracted_data)
        return extracted_data

    @staticmethod
    def _extract_field(field_name: str, field_def: Dict, content: str,
                      ontology: Dict, memory_pool: MemoryPool) -> Any:
        """
        æå–å•ä¸ªå­—æ®µ

        Args:
            field_name: å­—æ®µåç§°
            field_def: å­—æ®µå®šä¹‰
            content: æ–‡æ¡£å†…å®¹ï¼ˆå¯èƒ½æ˜¯å•ä¸ªchunkæˆ–è·¨chunkåˆå¹¶å†…å®¹ï¼‰
            ontology: æœ¬ä½“è®º
            memory_pool: è®°å¿†æ± 

        Returns:
            æå–çš„å­—æ®µå€¼
        """
        extraction_strategy = field_def["extraction_strategy"]
        strategy_def = ontology["extraction_strategies"].get(extraction_strategy)
        reference_content = content  # ä¿ç•™å®Œæ•´åŸæ–‡ä½œä¸ºå‚è€ƒæ¥æº

        if not strategy_def:
            memory_pool.log(f"    è­¦å‘Š: æœªæ‰¾åˆ°æå–ç­–ç•¥ '{extraction_strategy}'")
            return {"value": None, "reference": reference_content}

        # ç‰¹æ®Šå¤„ç†ï¼šåˆ†ç±»å‹å­—æ®µï¼ˆäº‹æ•…ç­‰çº§å’Œäº‹æ•…æ€§è´¨ï¼‰
        if extraction_strategy == "classify_with_options":
            # æ ¹æ®å­—æ®µåç§°ç¡®å®šä½¿ç”¨å“ªä¸ªé€‰é¡¹åˆ—è¡¨
            if field_name == "äº‹æ•…ç­‰çº§":
                options = InformationExtractor.ACCIDENT_LEVEL_OPTIONS
            elif field_name == "äº‹æ•…æ€§è´¨":
                options = InformationExtractor.ACCIDENT_NATURE_OPTIONS
            else:
                # å¦‚æœå­—æ®µå®šä¹‰ä¸­æœ‰é€‰é¡¹åˆ—è¡¨ï¼Œä½¿ç”¨å®ƒ
                options = field_def.get("options", [])

            if options:
                result = InformationExtractor._classify_with_options(content, field_name, options, memory_pool)
            else:
                memory_pool.log(f"    è­¦å‘Š: åˆ†ç±»å­—æ®µ '{field_name}' æ²¡æœ‰å®šä¹‰é€‰é¡¹åˆ—è¡¨")
                result = ""
            return {"value": result, "reference": reference_content}

        # ç‰¹æ®Šå¤„ç†ï¼šè´£ä»»äººå‘˜ï¼ˆéœ€è¦ç»¼åˆäººå‘˜ä¼¤äº¡æƒ…å†µå’Œè´£ä»»è®¤å®šä¸¤éƒ¨åˆ†ï¼‰
        if field_name == "è´£ä»»äººå‘˜":
            result = InformationExtractor._extract_responsible_persons(memory_pool, ontology)
            return {"value": result, "reference": reference_content}

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·¨chunkæå–
        if "source_categories" in field_def:
            source_categories = field_def["source_categories"]
            memory_pool.log(f"    è·¨chunkæå–ï¼Œæºç±»åˆ«: {', '.join(source_categories)}")
            content = InformationExtractor._collect_cross_chunk_content(source_categories, memory_pool)
            reference_content = content
            if not content:
                memory_pool.log(f"    è­¦å‘Š: æœªæ”¶é›†åˆ°ä»»ä½•å†…å®¹")
                # è¿”å›é»˜è®¤å€¼
                if field_def["type"] == "array":
                    default_value = []
                elif field_def["type"] in ["object", "text"]:
                    default_value = {}
                else:
                    default_value = ""
                return {"value": default_value, "reference": reference_content}

        # æ„å»º prompt
        prompt_template = strategy_def["prompt_template"]

        # å¤„ç† schemaï¼ˆå¦‚æœæœ‰ï¼‰
        schema_str = ""
        if "subfields" in field_def:
            schema_str = json.dumps(field_def["subfields"], ensure_ascii=False, indent=2)
        elif "item_schema" in field_def:
            schema_str = json.dumps(field_def["item_schema"], ensure_ascii=False, indent=2)

        # æ·»åŠ å­—æ®µè¯´æ˜ï¼ˆå¦‚æœæœ‰ï¼‰
        field_description = ""
        if "description" in field_def:
            field_description = f"\nã€å­—æ®µè¯´æ˜ã€‘{field_def['description']}\n"

        prompt = prompt_template.format(
            field_name=field_name,
            content=content[:15000],  # å¢åŠ å†…å®¹é•¿åº¦é™åˆ¶ä»¥æ”¯æŒè·¨chunk
            schema=schema_str
        )

        # å°†å­—æ®µè¯´æ˜æ’å…¥åˆ° prompt å¼€å¤´
        if field_description:
            prompt = field_description + prompt

        try:
            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ä¿¡æ¯æå–åŠ©æ‰‹ã€‚ä¸¥æ ¼å¤åˆ¶åŸæ–‡å†…å®¹ï¼Œä¸è¦æ”¹å†™æˆ–æ€»ç»“ã€‚åªè¿”å›æå–ç»“æœï¼Œä¸è¦æ·»åŠ è§£é‡Šã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.0,  # æ¸©åº¦è®¾ä¸º0ï¼Œç¡®ä¿ä¸€è‡´æ€§
                max_tokens=2000
            )

            result_str = response.choices[0].message.content.strip()

            # æ ¹æ®å­—æ®µç±»å‹è§£æç»“æœ
            field_type = field_def["type"]

            if field_type in ["array", "object"]:
                # å°è¯•è§£æ JSON
                try:
                    result = json.loads(result_str)
                except json.JSONDecodeError:
                    # å°è¯•æå– JSON éƒ¨åˆ†
                    code_block_match = re.search(r'```(?:json)?\s*\n?(.*?)\n?```', result_str, re.DOTALL)
                    if code_block_match:
                        result = json.loads(code_block_match.group(1))
                    else:
                        json_match = re.search(r'[\[\{].*[\]\}]', result_str, re.DOTALL)
                        if json_match:
                            result = json.loads(json_match.group())
                        else:
                            memory_pool.log(f"    è­¦å‘Š: æ— æ³•è§£æJSONç»“æœ")
                            result = [] if field_type == "array" else {}
            else:
                # å­—ç¬¦ä¸²æˆ–æ–‡æœ¬ç±»å‹
                result = result_str

            return {"value": result, "reference": reference_content}

        except Exception as e:
            memory_pool.log(f"    é”™è¯¯: æå–å¤±è´¥ - {e}")
            # è¿”å›é»˜è®¤å€¼
            if field_def["type"] == "array":
                default_value = []
            elif field_def["type"] == "object":
                default_value = {}
            else:
                default_value = ""
            return {"value": default_value, "reference": reference_content}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                   5ï¸âƒ£ OntologySerializer (åºåˆ—åŒ–å™¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OntologySerializer:
    """
    åºåˆ—åŒ–å™¨ï¼šæŒ‰æœ¬ä½“è®ºç»“æ„ç»„ç»‡æ•°æ®å¹¶åºåˆ—åŒ–ä¸º JSON

    è¾“å…¥: æå–çš„æ•°æ®
    è¾“å‡º: ç¬¦åˆæœ¬ä½“è®ºç»“æ„çš„ JSON æ–‡ä»¶
    """

    @staticmethod
    def serialize(memory_pool: MemoryPool, output_path: str):
        """
        åºåˆ—åŒ–ä¸º JSON

        Args:
            memory_pool: è®°å¿†æ± 
            output_path: è¾“å‡ºè·¯å¾„
        """
        memory_pool.log("OntologySerializer: å¼€å§‹åºåˆ—åŒ–æ•°æ®")

        extracted_data = memory_pool.get("extracted_data")
        ontology = memory_pool.get("ontology")
        document_path = memory_pool.get("document_path")

        # æ„å»ºæœ€ç»ˆçš„ JSON ç»“æ„
        final_json = {
            "_metadata": {
                "æœ¬ä½“è®ºç‰ˆæœ¬": ontology["ontology_metadata"]["version"],
                "æœ¬ä½“è®ºåç§°": ontology["ontology_metadata"]["name"],
                "æºæ–‡æ¡£": str(document_path),
                "å¤„ç†æ—¶é—´": datetime.now().isoformat(),
                "å¤„ç†å™¨": "OntologyAgent v1.0"
            }
        }

        # æŒ‰æœ¬ä½“è®ºçš„ä¼˜å…ˆçº§é¡ºåºç»„ç»‡æ•°æ®
        ontology_structure = ontology["ontology_structure"]
        sorted_categories = sorted(
            ontology_structure.items(),
            key=lambda x: x[1]["priority"]
        )

        for category_name, category_def in sorted_categories:
            if category_name in extracted_data:
                final_json[category_name] = extracted_data[category_name]
            else:
                # å¦‚æœæ²¡æœ‰æå–åˆ°ï¼Œåˆ›å»ºç©ºç»“æ„
                final_json[category_name] = {}

        # ä¿å­˜ä¸º JSON æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_json, f, ensure_ascii=False, indent=2)

        memory_pool.log(f"OntologySerializer: å·²ä¿å­˜åˆ° {output_path}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                       OntologyAgent (ä¸»æ§åˆ¶å™¨)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class OntologyAgent:
    """
    æœ¬ä½“è®ºé©±åŠ¨çš„äº‹æ•…æŠ¥å‘Šå¤„ç† Agent

    ä¸»æµç¨‹ï¼š
    1. åŠ è½½æœ¬ä½“è®º
    2. æå–æ ‡é¢˜å±‚çº§
    3. LLM è§„åˆ’æ‹†åˆ†æ–¹æ¡ˆ
    4. æ‰§è¡Œæ–‡æ¡£æ‹†åˆ†
    5. æå–ä¿¡æ¯
    6. åºåˆ—åŒ–ä¸º JSON
    """

    def __init__(self, ontology_path: str):
        """
        åˆå§‹åŒ– Agent

        Args:
            ontology_path: æœ¬ä½“è®ºæ–‡ä»¶è·¯å¾„
        """
        self.ontology_path = Path(ontology_path)
        self.ontology = self._load_ontology()

    def _load_ontology(self) -> Dict:
        """åŠ è½½æœ¬ä½“è®º"""
        print(f"\nğŸ“– åŠ è½½æœ¬ä½“è®º: {self.ontology_path}")
        with open(self.ontology_path, 'r', encoding='utf-8') as f:
            ontology = json.load(f)
        print(f"   ç‰ˆæœ¬: {ontology['ontology_metadata']['version']}")
        print(f"   åç§°: {ontology['ontology_metadata']['name']}")
        print(f"   ç±»åˆ«æ•°: {len(ontology['ontology_structure'])}")
        return ontology

    def process_document(self, md_file_path: str, output_dir: str) -> Dict:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£

        Args:
            md_file_path: Markdown æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            å¤„ç†ç»“æœ
        """
        md_path = Path(md_file_path)
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)

        print(f"\n{'='*80}")
        print(f"å¤„ç†æ–‡æ¡£: {md_path.name}")
        print(f"{'='*80}")

        # åˆå§‹åŒ–è®°å¿†æ± 
        memory_pool = MemoryPool()
        memory_pool.set("document_path", md_path)
        memory_pool.set("ontology", self.ontology)

        try:
            # è¯»å–æ–‡æ¡£
            memory_pool.log("è¯»å–æ–‡æ¡£å†…å®¹")
            with open(md_path, 'r', encoding='utf-8', errors='ignore') as f:
                md_content = f.read()
            memory_pool.set("document_content", md_content)
            print(f"   æ–‡æ¡£å¤§å°: {len(md_content):,} å­—ç¬¦")

            # 1ï¸âƒ£ æå–æ ‡é¢˜å±‚çº§
            print(f"\n1ï¸âƒ£  æå–æ ‡é¢˜å±‚çº§")
            headers = HeaderExtractor.extract(md_content, memory_pool)
            print(f"   âœ“ æå–åˆ° {len(headers)} ä¸ªæ ‡é¢˜")

            # 2ï¸âƒ£ LLM è§„åˆ’æ‹†åˆ†æ–¹æ¡ˆ
            print(f"\n2ï¸âƒ£  LLM è§„åˆ’æ‹†åˆ†æ–¹æ¡ˆ")
            split_plan = SplitPlanner.plan(memory_pool)
            print(f"   âœ“ ç”Ÿæˆ {len(split_plan)} ä¸ªæ‹†åˆ†chunk")

            # 3ï¸âƒ£ æ‰§è¡Œæ–‡æ¡£æ‹†åˆ†
            print(f"\n3ï¸âƒ£  æ‰§è¡Œæ–‡æ¡£æ‹†åˆ†")
            chunks = DocumentSplitter.split(memory_pool)
            print(f"   âœ“ æ‹†åˆ†å®Œæˆï¼Œå…± {len(chunks)} ä¸ªchunk")
            for chunk in chunks:
                print(f"      - {chunk['chunk_id']}: {chunk['char_count']:,} å­—ç¬¦")

            # 4ï¸âƒ£ æå–ä¿¡æ¯
            print(f"\n4ï¸âƒ£  æå–ä¿¡æ¯ (ä¸¥æ ¼å¤åˆ¶åŸæ–‡)")
            extracted_data = InformationExtractor.extract(memory_pool)
            print(f"   âœ“ æå–å®Œæˆï¼Œå…± {len(extracted_data)} ä¸ªç±»åˆ«")

            # 5ï¸âƒ£ åºåˆ—åŒ–ä¸º JSON
            print(f"\n5ï¸âƒ£  åºåˆ—åŒ–ä¸º JSON")
            json_output_path = output_path / f"{md_path.stem}_ontology.json"
            OntologySerializer.serialize(memory_pool, str(json_output_path))
            print(f"   âœ“ ä¿å­˜åˆ°: {json_output_path.name}")

            # stats: count source chars and output value chars (skip reference)
            def _sum_value_len(obj):
                if isinstance(obj, dict):
                    total = 0
                    if "value" in obj:
                        total += _sum_value_len(obj["value"])
                    for k, v in obj.items():
                        if k in ("value", "reference"):
                            continue
                        total += _sum_value_len(v)
                    return total
                if isinstance(obj, list):
                    return sum(_sum_value_len(x) for x in obj)
                if isinstance(obj, str):
                    return len(obj)
                return 0

            try:
                src_len = len(md_path.read_text(encoding="utf-8"))
                output_data = json.load(open(json_output_path, encoding="utf-8"))
                payload = {k: v for k, v in output_data.items() if k != "_metadata"}
                value_len = _sum_value_len(payload)
                print(f"   stats: src_chars={src_len}, output_value_chars={value_len}")
            except Exception as e:
                print(f"   stats calculation failed: {e}")

            # ä¿å­˜è®°å¿†æ± ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            memory_output_path = output_path / f"{md_path.stem}_memory.json"
            memory_pool.save_memory(str(memory_output_path))
            print(f"   âœ“ è®°å¿†æ± å·²ä¿å­˜: {memory_output_path.name}")

            print(f"\n{'='*80}")
            print(f"âœ“ å¤„ç†å®Œæˆ")
            print(f"{'='*80}")

            return {
                "success": True,
                "document": str(md_path),
                "output": str(json_output_path)
            }

        except Exception as e:
            memory_pool.log(f"å¤„ç†å¤±è´¥: {e}")
            print(f"\nâœ— å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

            return {
                "success": False,
                "document": str(md_path),
                "error": str(e)
            }

    def process_all_documents(self, dataset_dir: str, output_dir: str):
        """
        æ‰¹é‡å¤„ç† Dataset ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£

        Args:
            dataset_dir: Dataset ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        dataset_path = Path(dataset_dir)
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True, parents=True)

        print(f"\n{'='*80}")
        print(f"æ‰¹é‡å¤„ç†æ–‡æ¡£")
        print(f"{'='*80}")
        print(f"Datasetç›®å½•: {dataset_path}")
        print(f"è¾“å‡ºç›®å½•: {output_path}")

        # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶å¤¹
        doc_folders = [f for f in dataset_path.iterdir() if f.is_dir()]
        print(f"æ‰¾åˆ° {len(doc_folders)} ä¸ªæ–‡æ¡£æ–‡ä»¶å¤¹")
        print(f"{'='*80}")

        results = []
        success_count = 0
        fail_count = 0

        for i, doc_folder in enumerate(doc_folders, 1):
            print(f"\n[{i}/{len(doc_folders)}]")

            # æŸ¥æ‰¾ md æ–‡ä»¶
            md_files = [f for f in doc_folder.glob("*.md") if f.name.lower() != "readme.md"]

            if not md_files:
                print(f"  âœ— æœªæ‰¾åˆ° markdown æ–‡ä»¶")
                fail_count += 1
                continue

            md_file = md_files[0]

            # å¤„ç†æ–‡æ¡£
            result = self.process_document(str(md_file), str(output_path))
            results.append(result)

            if result["success"]:
                success_count += 1
            else:
                fail_count += 1

        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_path = output_path / "_processing_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"\n{'='*80}")
        print(f"æ‰¹é‡å¤„ç†å®Œæˆ")
        print(f"{'='*80}")
        print(f"  æˆåŠŸ: {success_count} ä¸ª")
        print(f"  å¤±è´¥: {fail_count} ä¸ª")
        print(f"  æ±‡æ€»æ–‡ä»¶: {summary_path.name}")
        print(f"{'='*80}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                               ä¸»ç¨‹åº
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """ä¸»ç¨‹åºå…¥å£"""

    # é…ç½®è·¯å¾„
    ONTOLOGY_PATH = r"C:\Users\Qzj\Desktop\projrct\MinerU\ontology_v2.json"
    DATASET_DIR = r"C:\Users\Qzj\Desktop\projrct\MinerU\Dataset"
    OUTPUT_DIR = r"C:\Users\Qzj\Desktop\projrct\MinerU\ontology_output_v2"

    # åˆ›å»º Agent
    agent = OntologyAgent(ontology_path=ONTOLOGY_PATH)

    # æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡æ¡£
    agent.process_all_documents(dataset_dir=DATASET_DIR, output_dir=OUTPUT_DIR)


if __name__ == "__main__":
    main()
